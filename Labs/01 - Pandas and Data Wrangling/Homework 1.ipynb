{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Task-1.-Compiling-Ebola-Data\"><span class=\"toc-item-num\">Task 1.&nbsp;&nbsp;</span>Compiling Ebola Data</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-2.-RNA-Sequences\"><span class=\"toc-item-num\">Task 2.&nbsp;&nbsp;</span>RNA Sequences</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-3.-Class-War-in-Titanic\"><span class=\"toc-item-num\">Task 3.&nbsp;&nbsp;</span>Class War in Titanic</a></div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'Data/'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Compiling Ebola Data\n",
    "\n",
    "The `DATA_FOLDER/ebola` folder contains summarized reports of Ebola cases from three countries (Guinea, Liberia and Sierra Leone) during the recent outbreak of the disease in West Africa. For each country, there are daily reports that contain various information about the outbreak in several cities in each country.\n",
    "\n",
    "Use pandas to import these data files into a single `Dataframe`.\n",
    "Using this `DataFrame`, calculate for *each country*, the *daily average per month* of *new cases* and *deaths*.\n",
    "Make sure you handle all the different expressions for *new cases* and *deaths* that are used in the reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "By looking at the data files from those 3 countries, we can see that there 3 types of new cases (confirmed, probable and suspected) that are represented every time by 3 different fields, and 1 field that corresponds to the number of new deaths. For those 3 countries, the data are represented in slightly different ways, using different terminologies. It thus makes sense to go over those 3 countries one after the other, clean, filter and uniformize the data, compute what we need (which is the average number of new cases and new deaths per day for each month) and then put those 3 countries and what we've computed together in a final DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corresponding code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "EBOLA_FOLDER = DATA_FOLDER + 'ebola/'\n",
    "CONFIRMED_NEW_CASES = 'New confirmed cases'\n",
    "PROBABLE_NEW_CASES = 'New probable cases'\n",
    "SUSPECTED_NEW_CASES = 'New suspected cases'\n",
    "NEW_DEATHS = 'New deaths'\n",
    "AVG_CONFIRMED_NEW_CASES = 'New confirmed cases (avg per day)'\n",
    "AVG_PROBABLE_NEW_CASES = 'New probable cases (avg per day)'\n",
    "AVG_SUSPECTED_NEW_CASES = 'New suspected cases (avg per day)'\n",
    "AVG_NEW_DEATHS = 'New deaths (avg per day)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utilitary functions\n",
    "def read_folder(date, folder):\n",
    "    df = pd.DataFrame()\n",
    "    for file in folder:\n",
    "        tmp_df = pd.read_csv(file,header=0,index_col=None,parse_dates=[date])\n",
    "        df = pd.concat([df, tmp_df])\n",
    "    return df\n",
    "\n",
    "def renamed_df(df):\n",
    "    # We rename the columns for the intermediate output\n",
    "    return df.rename(columns={\n",
    "        CONFIRMED_NEW_CASES: AVG_CONFIRMED_NEW_CASES,\n",
    "        PROBABLE_NEW_CASES: AVG_PROBABLE_NEW_CASES,\n",
    "        SUSPECTED_NEW_CASES: AVG_SUSPECTED_NEW_CASES,\n",
    "        NEW_DEATHS: AVG_NEW_DEATHS})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames creation\n",
    "For each country, we first have to read every CSV file and put their content into one big DataFrame. This will allow us to then work on the DataFrame (filter data, clean data, aggregate what we need, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_gui = glob.glob(EBOLA_FOLDER + \"guinea_data/*.csv\")\n",
    "file_lib = glob.glob(EBOLA_FOLDER + \"liberia_data/*.csv\")\n",
    "file_sie = glob.glob(EBOLA_FOLDER + \"sl_data/*.csv\")\n",
    "\n",
    "ebola_gui = read_folder('Date', file_gui)\n",
    "ebola_lib = read_folder('Date', file_lib)\n",
    "ebola_sie = read_folder('date', file_sie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guinea\n",
    "Here we want to extract the average per day for every month for all 3 types of new cases and for new deaths in Guinea. What we have to do is to filter the data for each case (we have 4 cases) we want to have, get each time the corresponding number of new cases or deaths, and merge those 4 DataFrames into 1 by the date. Once this is done, we can extract the month and year from the date, group by the month and year, and compute the average as an aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_guinea_specific_df(description, new_col_name):\n",
    "    res = ebola_gui.where(lambda row: row['Description'] == description)\n",
    "    df = res[pd.notnull(res['Description'])]\n",
    "    df = df[['Totals', 'Date']]\n",
    "    df = df.rename(columns = {'Totals': new_col_name})\n",
    "    return df\n",
    "    \n",
    "new_cases_confirmed = get_guinea_specific_df('New cases of confirmed', CONFIRMED_NEW_CASES)\n",
    "new_cases_probable = get_guinea_specific_df('New cases of probables', PROBABLE_NEW_CASES)\n",
    "new_cases_suspect = get_guinea_specific_df('New cases of suspects', SUSPECTED_NEW_CASES)\n",
    "# Note: This makes the first file (2014-08-04) disappear because it is registered under \"New deaths registered today\"\n",
    "# and not \"New deaths registered\" like the others.\n",
    "new_deaths_registered = get_guinea_specific_df('New deaths registered', NEW_DEATHS)\n",
    "\n",
    "# Put all things together\n",
    "guinea_df = new_cases_confirmed.merge(new_cases_probable).merge(new_cases_suspect).merge(new_deaths_registered)\n",
    "guinea_df['Country'] = 'Guinea'\n",
    "guinea_df['Date'] = guinea_df.Date.dt.strftime(\"%B %Y\")\n",
    "guinea_df = guinea_df.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "# Group by to compute the average per day for each month\n",
    "guinea_df = guinea_df.groupby(['Date', 'Country']).mean()\n",
    "\n",
    "renamed_df(guinea_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liberia\n",
    "The idea here is similar. The big difference is that there are numbers that seem wrongly reported in December (you can see huge numbers, bigger than 1000), so we want to filter them out so that they don't mess up our data. To do that we choose a threshold (500) and if we find a value bigger than the threshold, we remove the row because it is very likely that this is a wrongly reported value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_liberia_specific_df(variable, new_col_name):\n",
    "    res = ebola_lib.where(lambda row: row['Variable'] == variable)\n",
    "    df = res[pd.notnull(res['Variable'])]\n",
    "    df = df[pd.isnull(df['National']) | (df['National'] < 500.0)] # Remove wrongly reported data, but keep NaN for now\n",
    "    df = df[['National', 'Date']]\n",
    "    df = df.rename(columns = {'National': new_col_name})\n",
    "    return df\n",
    "\n",
    "new_cases_confirmed = get_liberia_specific_df('New case/s (confirmed)', CONFIRMED_NEW_CASES)\n",
    "new_cases_probable = get_liberia_specific_df('New Case/s (Probable)', PROBABLE_NEW_CASES)\n",
    "new_cases_suspect = get_liberia_specific_df('New Case/s (Suspected)', SUSPECTED_NEW_CASES)\n",
    "new_deaths_registered = get_liberia_specific_df('Newly reported deaths', NEW_DEATHS)\n",
    "\n",
    "# Put all things together\n",
    "liberia_df = new_cases_confirmed.merge(new_cases_probable, on='Date').merge(new_cases_suspect, on='Date').merge(new_deaths_registered, on='Date')\n",
    "liberia_df['Country'] = 'Liberia'\n",
    "liberia_df['Date'] = liberia_df.Date.dt.strftime(\"%B %Y\")\n",
    "liberia_df = liberia_df.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "# Group by to compute the average per day for each month\n",
    "liberia_df = liberia_df.groupby(['Date', 'Country']).mean()\n",
    "\n",
    "renamed_df(liberia_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sierra Leone\n",
    "Again, the idea is the same. But this time, we don't have a number that gives us exactly the new number of deaths. So instead, we are using 'etc_new_deaths' for computing the number of deaths per day; 'etc' means 'Ebola Treatment Center', so those seem to be the deaths happening in the treatment center.\n",
    "Another solution would have been to use 'death_confirmed', 'death_suspected' and 'death_probable', but those values are cumulative, so we would have had to compute the differences every day, and be sure that we have data every day to have accurate values, but this is not the case here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sl_specific_df(variable, new_col_name):\n",
    "    res = ebola_sie.where(lambda row: row['variable'] == variable)\n",
    "    df = res[pd.notnull(res['variable'])]\n",
    "    df = df[['National', 'date']]\n",
    "    df = df.rename(columns = {'National': new_col_name})\n",
    "    return df\n",
    "\n",
    "new_cases_confirmed = get_sl_specific_df('new_confirmed', CONFIRMED_NEW_CASES)\n",
    "new_cases_probable = get_sl_specific_df('new_probable', PROBABLE_NEW_CASES)\n",
    "new_cases_suspect = get_sl_specific_df('new_suspected', SUSPECTED_NEW_CASES)\n",
    "new_deaths_registered = get_sl_specific_df('etc_new_deaths', NEW_DEATHS)\n",
    "\n",
    "# Put all things together\n",
    "sl_df = new_cases_confirmed.merge(new_cases_probable, on='date').merge(new_cases_suspect, on='date').merge(new_deaths_registered, on='date')\n",
    "sl_df['Country'] = 'Sierra Leone'\n",
    "sl_df['date'] = sl_df.date.dt.strftime(\"%B %Y\")\n",
    "sl_df = sl_df.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "# Group by to compute the average per day for each month\n",
    "sl_df = sl_df.groupby(['date', 'Country']).mean()\n",
    "\n",
    "renamed_df(sl_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put everything together\n",
    "Finally, we just want to concatenate everything together and rename the different columns accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the values for the 3 countries and rename columns accordingly\n",
    "final_df = pd.concat([guinea_df, liberia_df, sl_df])\n",
    "final_df = renamed_df(final_df)\n",
    "final_df = final_df.fillna('unknown')\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. RNA Sequences\n",
    "\n",
    "In the `DATA_FOLDER/microbiome` subdirectory, there are 9 spreadsheets of microbiome data that was acquired from high-throughput RNA sequencing procedures, along with a 10<sup>th</sup> file that describes the content of each. \n",
    "\n",
    "Use pandas to import the first 9 spreadsheets into a single `DataFrame`.\n",
    "Then, add the metadata information from the 10<sup>th</sup> spreadsheet as columns in the combined `DataFrame`.\n",
    "Make sure that the final `DataFrame` has a unique index and all the `NaN` values have been replaced by the tag `unknown`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have 9 spreedsheets containing microbiome data. This data can be seen as a single table containing 2 columns, the `NAME` and the `COUNT`. Thus, we simply read those files and concatenate them into a single dataframe representing our table with 2 columns.\n",
    "\n",
    "Given that the file does not contain headers, we ignore the first row (which is not a header) and give a name to the columns with the `header=None` and `names=['NAME', 'COUNT']` parameters.\n",
    "\n",
    "We also add a new `BARCODE` column to the dataframe, which is the name corresponding to the column name in the metadata file. We give it the same name in order to have normalized dataframes that can be merged on columns with the same name.\n",
    "\n",
    "Finally, we read the metadata file into a dataframe that we merge with our initial dataframe, we replace NA values with the `unknown` value and place an index on `[BARCODE, NAME]` in order to have unique indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corresponding Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MICROBIOME_FOLDER = DATA_FOLDER + 'microbiome/'\n",
    "mid_files = glob.glob(MICROBIOME_FOLDER + \"MID*.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial empty dataframe\n",
    "mid = pd.DataFrame()\n",
    "for idx in range(1, len(mid_files)+1):\n",
    "    file_name = 'MID' + str(idx)\n",
    "    # We give appropriate names to colummns\n",
    "    tmp_mid = pd.read_excel(MICROBIOME_FOLDER+file_name+'.xls', names=['NAME', 'COUNT'], header=None)\n",
    "    # We add a column \"BARCODE\" corresponding to the name of the file being read\n",
    "    tmp_mid['BARCODE'] = file_name\n",
    "    # We concatenate the values to create a single dataframe\n",
    "    mid = pd.concat([mid, tmp_mid])\n",
    "\n",
    "# We read the metadata file and merge it to the MID dataframe where columns have the same name\n",
    "metadata = pd.read_excel(MICROBIOME_FOLDER + 'metadata.xls')\n",
    "# We create an unique index on BARCODE and TAXON and replace NA values by 'unknown'\n",
    "final_df = pd.merge(mid, metadata).fillna('unknown').set_index(['BARCODE', 'NAME'])\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Class War in Titanic\n",
    "\n",
    "Use pandas to import the data file `Data/titanic.xls`. It contains data on all the passengers that travelled on the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(filename=DATA_FOLDER+'/titanic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following questions state clearly your assumptions and discuss your findings:\n",
    "1. Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "2. Plot histograms for the *travel class*, *embarkation port*, *sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "3. Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*.\n",
    "4. For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*.\n",
    "5. Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*.\n",
    "6. Create 2 equally populated *age categories* and calculate survival proportions by *age category*, *travel class* and *sex*. Present your results in a `DataFrame` with unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_excel('Data/titanic.xls')\n",
    "titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) In the follow output we can see the different type of attributes with their unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in titanic.columns:\n",
    "    print(\"%s is of type %s\" % (c,titanic[c].dtype))\n",
    "    print(titanic[c].unique())\n",
    "    print(\"\\n ------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above output we can see that travel class, surviving, sex and embarked have some limited possibilities,\n",
    "which make them good candidate for categorical type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change travel class to categorical type with more significant label\n",
    "titanic['pclass'] = titanic['pclass'].apply(lambda x: {1:'1st',2:'2nd',3:'3rd'}[x]).astype('category')\n",
    "#change surviving attribute to categorical type\n",
    "titanic['survived'] = titanic['survived'].astype('category')\n",
    "#change sex attribute to categorical type\n",
    "titanic['sex'] = titanic['sex'].astype('category')\n",
    "#change embarked attribute to categorical type with more significant label\n",
    "ports = {'C':'Cherbourg','Q':'Queenstown','S':'Southampton'}\n",
    "keys = ['C','Q','S']\n",
    "titanic['embarked'] = titanic['embarked'].apply(lambda x : ports.get(x,ports[keys[randint(0,2)]]))\n",
    "titanic.pclass.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Histogram plot for travel class, embarkation port, sex and age by decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_pclass = titanic['pclass']\n",
    "hist_pclass.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_emb = titanic['embarked'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_sex = titanic['sex'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_age = titanic['age'].plot.hist(bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Proportion of passengers by cabin floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume that ine value 'A10' the floor is 'A'\n",
    "# First we take the cabin columns as string, map them on the first letter and the\n",
    "cabin_plot = titanic['cabin'].astype(str).apply(lambda x : 'unkown' if x=='nan' else x[0]).copy()\n",
    "cabin_plot.value_counts().plot.pie(autopct='%.2f',figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is so much unkown we plot also the pie chart without them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same values but without the unkown\n",
    "cabin_plot = titanic['cabin'].dropna().astype(str).apply(lambda x : x[0]).copy()\n",
    "cabin_plot.value_counts().plot.pie(autopct='%.2f',figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Proportion of the passengers that survived for each travel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter needed columns\n",
    "survived_data = titanic[['pclass','survived']].copy()\n",
    "# Created columns for people who died\n",
    "survived_data['died'] = 1 - survived_data['survived'].astype('int')\n",
    "# Convert survived column to int type\n",
    "survived_data['survived'] = survived_data['survived'].astype('int')\n",
    "# Group by travel class on a sum and transpose (for ploting to works)\n",
    "survived_plot = survived_data.groupby('pclass').sum().transpose()\n",
    "# Subplot data on 3 pie charts\n",
    "survived_plot.plot.pie(subplots=True,autopct='%.2f',figsize=(12, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Proportion of the passengers that survived by travel class and sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter needed columns\n",
    "survived_sex_data = titanic[['pclass','sex','survived']].copy()\n",
    "#convert survived categorical type to int\n",
    "survived_sex_data['survived'] = survived_sex_data['survived'].astype('int')\n",
    "#group by class of pairs and plot (we assume seaborn is allowed since it was used in the intro to Pandas I)\n",
    "sns.barplot(x='pclass',y='survived',hue='sex',data=survived_sex_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Survival proportions by age category, travel class and sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter needed columns\n",
    "prop_data = titanic[['pclass','survived','age','sex']].copy().sort_values(['age'])\n",
    "data_age = prop_data['age'].dropna()\n",
    "# Compute median\n",
    "median_val = np.median(data_age)\n",
    "# Create new age category\n",
    "prop_data['age_cat'] = data_age.apply(lambda x : 'young' if x < median_val else 'old')\n",
    "# Create a accumulator columns to compute proportion\n",
    "prop_data['acc'] = 1\n",
    "# Create new class with age category, travel class and sex\n",
    "prop_data['class'] = prop_data['age_cat'].astype(str) + \" \" + prop_data['pclass'].astype(str) + \" \" + prop_data['sex'].astype(str)\n",
    "# Filter out non needed columns\n",
    "prop_df = prop_data[['survived','class']].copy()\n",
    "# Converting categorical type to int\n",
    "prop_df['survived'] = prop_df['survived'].astype('int')\n",
    "# Group by new class\n",
    "prop_data['survived'] = prop_data['survived'].astype('int')\n",
    "final_data = prop_data[['age_cat','pclass','sex','survived','acc']].groupby(['age_cat','pclass','sex']).sum()\n",
    "final_data['proportion_survived'] = final_data['survived'] / final_data['acc']\n",
    "final_data[['proportion_survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
