{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, cross_validate\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.utils import column_or_1d\n",
    "from sklearn import linear_model\n",
    "import scipy as sc\n",
    "from scipy.linalg import block_diag\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "EARNINGS_STEP = 5000\n",
    "AGE_STEP = 5\n",
    "EDUC_STEP = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV and put 0.0 values to NaN\n",
    "lalonde_df = pd.read_csv('lalonde.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. A naive analysis\n",
    "\n",
    "Compare the distribution of the outcome variable (`re78`) between the two groups, using plots and numbers.\n",
    "To summarize and compare the distributions, you may use the techniques we discussed in lectures 4 (\"Read the stats carefully\") and 6 (\"Data visualization\").\n",
    "\n",
    "What might a naive \"researcher\" conclude from this superficial analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "We decided to show this by simply computing the average for both group. Also, we thought that showing how much money people in both group make by assigning them to categories (range of money they are making) would help visualize the differences. We chose 5000 as our step size for earnings because it allows to split the people in a meaningful number of categories.\n",
    "\n",
    "#### Assumptions\n",
    "\n",
    "We assumed that the people earning 0.0 are unemployed (as it was hinted on Mattermost by a TA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_distribution(df, attr, step, zero_meaning=None):\n",
    "    # Add a category column\n",
    "    attr_df = df.copy()\n",
    "    attr_df['category'] = attr_df[attr] // step\n",
    "    \n",
    "    # Put -1 as a category for zero values if it means something special\n",
    "    if zero_meaning:\n",
    "        attr_df.loc[attr_df[attr] == 0, 'category'] = -1\n",
    "\n",
    "    # Compute the labels\n",
    "    categories = np.sort(attr_df['category'].unique()) * step\n",
    "    labels = []\n",
    "    \n",
    "    # Add 'zero_meaning' label if any value is at 0.0\n",
    "    if zero_meaning and (attr_df[attr] == 0).any():\n",
    "        labels.append(zero_meaning)\n",
    "    \n",
    "    for i in range(len(categories)):\n",
    "        current = categories[i]\n",
    "        if current < 0:\n",
    "            continue\n",
    "            \n",
    "        if i == len(categories) - 1:\n",
    "            nextt = '+'\n",
    "        else:\n",
    "            nextt = str(int(categories[i+1]))\n",
    "\n",
    "        labels.append('[%d, %s]' % (current, nextt)) \n",
    "\n",
    "    # Show with a barplot the average (again) with the uncertainty\n",
    "    g = sb.countplot(y='category', hue='treat', data=attr_df)\n",
    "    g.set(yticklabels=labels)\n",
    "    \n",
    "def plot_distribution_binary(df, attr):\n",
    "    # Show with a barplot the average (again) with the uncertainty\n",
    "    g = sb.countplot(y=attr, hue='treat', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean in earnings (1978) for both treatments\n",
    "print(\"Mean of earnings (full dataset):\")\n",
    "print(\"Control group:\", np.mean(lalonde_df[lalonde_df['treat'] == 0]['re78']))\n",
    "print(\"Treated group:\", np.mean(lalonde_df[lalonde_df['treat'] == 1]['re78']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the distribution of earnings\n",
    "plot_distribution(lalonde_df, 're78', EARNINGS_STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "A naive \"researcher\" would say that the control group people earn more in average, thus the treatment didn't do anything (and it would even be better not to do it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. A closer look at the data\n",
    "\n",
    "You're not naive, of course (and even if you are, you've learned certain things in ADA), so you aren't content with a superficial analysis such as the above.\n",
    "You're aware of the dangers of observational studies, so you take a closer look at the data before jumping to conclusions.\n",
    "\n",
    "For each feature in the dataset, compare its distribution in the treated group with its distribution in the control group, using plots and numbers.\n",
    "As above, you may use the techniques we discussed in class for summarizing and comparing the distributions.\n",
    "\n",
    "What do you observe?\n",
    "Describe what your observations mean for the conclusions drawn by the naive \"researcher\" from his superficial analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "To compare both distributions, we use the same kind of plot we used earlier but for all features. This allows us to see the inequalities (discussed in the results).\n",
    "\n",
    "#### Assumptions\n",
    "\n",
    "We assumed (again) that the people earning 0.0 are unemployed (as it was hinted on Mattermost by a TA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_distribution(lalonde_df, 'age', AGE_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_distribution(lalonde_df, 'educ', EDUC_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_distribution(lalonde_df, 're74', EARNINGS_STEP, 'Unemployed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_distribution(lalonde_df, 're75', EARNINGS_STEP, 'Unemployed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_distribution_binary(lalonde_df, 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_distribution_binary(lalonde_df, 'hispan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_distribution_binary(lalonde_df, 'married')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_distribution_binary(lalonde_df, 'nodegree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "The dataset is unbalanced because we can see that the people in the control group and in the treated group are not similar, we can see that the distributions are quite different (e.g. lots of \"non-black people\" in the control group compared to the treated group). This means that the conclusions made by the \"naive\" researcher are most likely wrong because it is not based on a balanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. A propensity score model\n",
    "\n",
    "Use logistic regression to estimate propensity scores for all points in the dataset.\n",
    "You may use `sklearn` to fit the logistic regression model and apply it to each data point to obtain propensity scores:\n",
    "\n",
    "```python\n",
    "from sklearn import linear_model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "```\n",
    "\n",
    "Recall that the propensity score of a data point represents its probability of receiving the treatment, based on its pre-treatment features (in this case, age, education, pre-treatment income, etc.).\n",
    "To brush up on propensity scores, you may read chapter 3.3 of the above-cited book by Rosenbaum or [this article](https://drive.google.com/file/d/0B4jctQY-uqhzTlpBaTBJRTJFVFE/view).\n",
    "\n",
    "Note: you do not need a train/test split here. Train and apply the model on the entire dataset. If you're wondering why this is the right thing to do in this situation, recall that the propensity score model is not used in order to make predictions about unseen data. Its sole purpose is to balance the dataset across treatment groups.\n",
    "(See p. 74 of Rosenbaum's book for an explanation why slight overfitting is even good for propensity scores.\n",
    "If you want even more information, read [this article](https://drive.google.com/file/d/0B4jctQY-uqhzTlpBaTBJRTJFVFE/view).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "We can compute the weights of the different features by using logistic regression, with X being all features except 'id', 'treat' and 're78' and y being 'treat'. Once we have that, we can compute an associated score (the expected 'treat', as a real number), standardize it and then derive the probability (for every point) of being treated (i.e. the propensity score); to do so, we take the sigmoid of the score, which is smoothly mapping the score into the [0, 1] range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Compute 'w' in order to get the propensity scores\n",
    "logistic = linear_model.LogisticRegression()\n",
    "X = lalonde_df.drop(['id', 'treat', 're78'], axis=1)\n",
    "y = lalonde_df['treat']\n",
    "model = logistic.fit(X, y)\n",
    "w = logistic.coef_\n",
    "\n",
    "print(\"Weights of features:\")\n",
    "print(X.columns.values)\n",
    "print(w)\n",
    "\n",
    "# Get the probability of being treated\n",
    "prob = model.predict_proba(X)[:, 1]\n",
    "\n",
    "# DataFrame with propensity score\n",
    "propensity_df = lalonde_df.copy()\n",
    "propensity_df['prop_score'] = prob\n",
    "\n",
    "# Showing some from the treated group and some from the control group\n",
    "pd.concat([propensity_df.head(5), propensity_df.tail(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "From the computed weights, we can derive how \"important\" a feature is in order to tell if the person is in the treated group or not. For example, the most important feature seems to be the feature 'black'; indeed, it seems that it is way more likely that the person is in the treated group if this person is black.\n",
    "\n",
    "Also, if you look at the propensity score in the DataFrame, you can see that indeed, in general, a bigger score means most likely that the person is in the treated group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Balancing the dataset via matching\n",
    "\n",
    "Use the propensity scores to match each data point from the treated group with exactly one data point from the control group, while ensuring that each data point from the control group is matched with at most one data point from the treated group.\n",
    "(Hint: you may explore the `networkx` package in Python for predefined matching functions.)\n",
    "\n",
    "Your matching should maximize the similarity between matched subjects, as captured by their propensity scores.\n",
    "In other words, the sum (over all matched pairs) of absolute propensity-score differences between the two matched subjects should be minimized.\n",
    "\n",
    "After matching, you have as many treated as you have control subjects.\n",
    "Compare the outcomes (`re78`) between the two groups (treated and control).\n",
    "\n",
    "Also, compare again the feature-value distributions between the two groups, as you've done in part 2 above, but now only for the matched subjects.\n",
    "What do you observe?\n",
    "Are you closer to being able to draw valid conclusions now than you were before?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_treated = np.where(propensity_df['treat'] == 1)[0]\n",
    "idx_non_treated = np.where(propensity_df['treat'] == 0)[0]\n",
    "\n",
    "# Create the graph by adding all indices as nodes\n",
    "G = nx.Graph()\n",
    "for idx in propensity_df.index:\n",
    "    G.add_node(idx)\n",
    "    \n",
    "# Add edges between the nodes with the weight being 1 minus the diff in propensity score\n",
    "# (since there is no 'min_weight_matching' function)\n",
    "for idx_t in idx_treated:\n",
    "    for idx_non_t in idx_non_treated:\n",
    "        w = 1 - (np.abs(propensity_df['prop_score'].iloc[idx_t] - propensity_df['prop_score'].iloc[idx_non_t]))\n",
    "        G.add_edge(idx_t, idx_non_t, weight=w)   \n",
    "        \n",
    "# All matchings (as a dictionary)\n",
    "matchings = nx.max_weight_matching(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to remove the similar pairs (e.g. (x, y) and (y, x))\n",
    "clean_matchings = {}\n",
    "for k,v in matchings.items():\n",
    "    if not v in clean_matchings:\n",
    "        clean_matchings[k] = v\n",
    "\n",
    "# Create a list of pairs (matchings)\n",
    "clean_matchings = list(clean_matchings.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices for control group and treated group\n",
    "idx_treated_matched = [x[0] if x[0] in idx_treated else x[1] for x in clean_matchings]\n",
    "idx_non_treated_matched = [x[0] if x[0] in idx_non_treated else x[1] for x in clean_matchings]\n",
    "\n",
    "# Create new DF based on those indices\n",
    "treated_df = propensity_df.loc[idx_treated_matched]\n",
    "non_treated_df = propensity_df.loc[idx_non_treated_matched]\n",
    "matched_df = pd.concat([treated_df, non_treated_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a new DF with only the matchings, let's see the distribution of the different features again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(matched_df, 'age', AGE_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(matched_df, 'educ', EDUC_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_binary(matched_df, 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_binary(matched_df, 'hispan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_binary(matched_df, 'married')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_binary(matched_df, 'nodegree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(matched_df, 're74', EARNINGS_STEP, 'Unemployed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_distribution(matched_df, 're75', EARNINGS_STEP, 'Unemployed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Balancing the groups further\n",
    "\n",
    "Based on your comparison of feature-value distributions from part 4, are you fully satisfied with your matching?\n",
    "Would you say your dataset is sufficiently balanced?\n",
    "If not, in what ways could the \"balanced\" dataset you have obtained still not allow you to draw valid conclusions?\n",
    "\n",
    "Improve your matching by explicitly making sure that you match only subjects that have the same value for the problematic feature.\n",
    "Argue with numbers and plots that the two groups (treated and control) are now better balanced than after part 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "As you can see on the plot, the 'black' feature is still unbalanced compared to the others. So when creating the edges in the graph, we should force that both nodes correspond to a person with the same 'black' feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_treated = np.where(propensity_df['treat'] == 1)[0]\n",
    "idx_non_treated = np.where(propensity_df['treat'] == 0)[0]\n",
    "\n",
    "# Create the graph by adding all indices as nodes\n",
    "G = nx.Graph()\n",
    "for idx in propensity_df.index:\n",
    "    G.add_node(idx)\n",
    "    \n",
    "# Add edges between the nodes with the weight being 1 minus the diff in propensity score\n",
    "# (since there is no 'min_weight_matching' function)\n",
    "for idx_t in idx_treated:\n",
    "    for idx_non_t in idx_non_treated:\n",
    "        # Make sure we can only match those with the same 'black' feature\n",
    "        if propensity_df['black'].iloc[idx_t] == propensity_df['black'].iloc[idx_non_t]:\n",
    "            w = 1 - (np.abs(propensity_df['prop_score'].iloc[idx_t] - propensity_df['prop_score'].iloc[idx_non_t]))\n",
    "            G.add_edge(idx_t, idx_non_t, weight=w)   \n",
    "        \n",
    "# All matchings (as a dictionary)\n",
    "matchings = nx.max_weight_matching(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to remove the similar pairs (e.g. (x, y) and (y, x))\n",
    "clean_matchings = {}\n",
    "for k,v in matchings.items():\n",
    "    if not v in clean_matchings:\n",
    "        clean_matchings[k] = v\n",
    "\n",
    "# Create a list of pairs (matchings)\n",
    "clean_matchings = list(clean_matchings.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices for control group and treated group\n",
    "idx_treated_matched = [x[0] if x[0] in idx_treated else x[1] for x in clean_matchings]\n",
    "idx_non_treated_matched = [x[0] if x[0] in idx_non_treated else x[1] for x in clean_matchings]\n",
    "\n",
    "# Create new DF based on those indices\n",
    "treated_df = propensity_df.loc[idx_treated_matched]\n",
    "non_treated_df = propensity_df.loc[idx_non_treated_matched]\n",
    "matched_df = pd.concat([treated_df, non_treated_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distributions now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_distribution(matched_df, 'age', AGE_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(matched_df, 'educ', EDUC_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_distribution_binary(matched_df, 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_distribution_binary(matched_df, 'hispan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_distribution_binary(matched_df, 'married')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_distribution_binary(matched_df, 'nodegree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_distribution(matched_df, 're74', EARNINGS_STEP, 'Unemployed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_distribution(matched_df, 're75', EARNINGS_STEP, 'Unemployed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. A less naive analysis\n",
    "\n",
    "Compare the outcomes (`re78`) between treated and control subjects, as you've done in part 1, but now only for the matched dataset you've obtained from part 5.\n",
    "What do you conclude about the effectiveness of the job training program?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean of earnings (balanced dataset):\")\n",
    "print(\"Control group:\", np.mean(non_treated_df['re78']))\n",
    "print(\"Treated group:\", np.mean(treated_df['re78']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_distribution(matched_df, 're78', EARNINGS_STEP, 'Unemployed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, cross_validate\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, IncrementalPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to build a classifier of news to directly assign them to 20 news categories. Note that the pipeline that you will build in this exercise could be of great help during your project if you plan to work with text!\n",
    "\n",
    "1. Load the 20newsgroup dataset. It is, again, a classic dataset that can directly be loaded using sklearn ([link](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)).  \n",
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), short for term frequencyâ€“inverse document frequency, is of great help when if comes to compute textual features. Indeed, it gives more importance to terms that are more specific to the considered articles (TF) but reduces the importance of terms that are very frequent in the entire corpus (IDF). Compute TF-IDF features for every article using [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Then, split your dataset into a training, a testing and a validation set (10% for validation and 10% for testing). Each observation should be paired with its corresponding label (the article category).\n",
    "\n",
    "2. Train a random forest on your training set. Try to fine-tune the parameters of your predictor on your validation set using a simple grid search on the number of estimator \"n_estimators\" and the max depth of the trees \"max_depth\". Then, display a confusion matrix of your classification pipeline. Lastly, once you assessed your model, inspect the `feature_importances_` attribute of your random forest and discuss the obtained results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation and assumptions\n",
    "\n",
    "#### Explanation\n",
    "\n",
    "In order to create a Random Forest to classify the news articles, we start by downloading all the data.\n",
    "\n",
    "We then use the `TfidfVectorizer` to turn the data into vectors in order to be able to insert it in our random forest.\n",
    "\n",
    "Then, we split the data into 2 sets, a training set and a test set. The test set is 10% of the size of the entire dataset. \n",
    "\n",
    "After that, we use grid search and cross-validation to fine tune our model and select the best parameters for max_depth and n_estimators (which is the depth of the trees and the number of trees in the random forest, respectivel)\n",
    "\n",
    "Finally, when we obtain a good-enough accuracy in our validation phase, we assess the model and test the accuracy on the test set. We also plot a confusion matrix where we can see the accuracy for each category\n",
    "\n",
    "#### Assumptions\n",
    "\n",
    "We assumed that the training data was well balanced (and verified that it was) before creating the test and training sets. It the data was not balanced, we could have used a weighted or stratified sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data retrieving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We directly load the data using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check that the data is well balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(19):\n",
    "    print(np.count_nonzero(newsgroups.target == i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we convert the text to vectors taking care of setting a *max_features* parameter in order to have the same number of features in the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "newsgroups.vectors = vectorizer.fit_transform(newsgroups.data)\n",
    "print('Set shape:', newsgroups.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups.vectors, newsgroups.target, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORING = ['accuracy', 'neg_mean_squared_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runCV(clf, X_train, y_train, k):\n",
    "    scores = cross_validate(clf, X_train, y_train, cv=k, scoring=SCORING, return_train_score=False)\n",
    "    print_scores(scores)\n",
    "    return scores\n",
    "        \n",
    "def print_scores(scores):\n",
    "    print('Scores')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores['test_accuracy'].mean(), scores['test_accuracy'].std() * 2))\n",
    "    print(\"RMSE: %0.2f (+/- %0.2f)\" % (np.sqrt(-scores['test_neg_mean_squared_error']).mean(), scores['test_neg_mean_squared_error'].std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With **k-fold cross validation** and **grid_search**, we found that good parameters are:\n",
    "\n",
    "* n_estimators = 400\n",
    "* max_depth = 100\n",
    "\n",
    "Further fine-tuning can be done but we estimated that those parameters were enough considering the long running-time involved in finding the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuning(depths, estimators):\n",
    "    best_depth = 0;\n",
    "    best_estimators = 0;\n",
    "    best_acc = 0;\n",
    "\n",
    "    for max_depth in depths:\n",
    "        for n_estimators in estimators:\n",
    "            clf = RandomForestClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            scores = runCV(clf, X_train, newsgroups_train.target, 7)\n",
    "            acc = scores['test_accuracy'].mean()\n",
    "\n",
    "            if acc > best_acc:\n",
    "                best_depth = max_depth\n",
    "                best_estimators = n_estimators\n",
    "                best_acc = acc\n",
    "                \n",
    "    print('Best parameters are (accuracy of', best_acc, '):')\n",
    "    print('Depth:', best_depth, 'n_estimators:', best_estimators)\n",
    "    \n",
    "    return best_depth, best_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning([30, 50, 70, 90, 100], [50, 150, 200, 300, 350, 400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 100\n",
    "n_estimators = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, n_jobs=-1, random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We obtain an accuracy of 85%, which is pretty good so we stop the research here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, pred)\n",
    "cm = pd.DataFrame(cm, index=newsgroups.target_names, columns=newsgroups.target_names)\n",
    "cm = cm.div(cm.sum(axis=1), axis=0)\n",
    "plt.figure(figsize=(15, 15))\n",
    "sb.heatmap(cm, square=True, annot=True, linewidths=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our model is accurate most of the time, some categories like talk_religion_misc are more difficult to classify than caregories like sci_space or sport_hockey. This is surely due to highly specific words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_features = np.flip(np.sort(clf.feature_importances_), axis=0)\n",
    "sorted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(sorted_features == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some features have an importance of 0. This means that those features are not used at all to classify the articles. Those features correspond to words that are common in every article. Like \"point\", \"comma\", \"the\", etc.\n",
    "\n",
    "The number of features that have a weight of 0 is really high. A lot of features do not have any influence in the decision of the category.\n",
    "\n",
    "After seeing this, we can decide to use a dimensionality reduction method (like PCA or truncatedSVD) to delete unnecessary features and accelerate the process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
